### 辞書のコストの設定方法
　後述する理由で壊れているため、最長一致で分割されるように辞書のコストを設定する（予定）


### 壊れている機能


openjtalkの辞書はunidic-csj(たぶん2.12.2)とnaist-jdicをマージして使われており、形態素解析、短単位解析能力が機能しなくなっている。

主にコストの調整がされていないことが原因。

kanji-*gram-*.csvの内容を見比べてみて、4行目の数字がコストで小さい順で出やすいのだが、unidicとnaist-jdicで数値の差が大きいことがわかる
特にunidicのよりnaist-jdicの方が出やすく、unidicはほとんど使われていないパターンのほうが多そう

また、現状コストを手動調整する以外に方法はない。

openjtalk 1.11でunidicの追加がされており、辞書のみそれ以前のバージョンに戻すことも可能
ただopenjtalk 1.11には辞書にフィラー項目が追加されており、それがないと未知語を正しく読めない。

pyopenjtalk-plusではその問題を懸念してフィラー項目のみ別ファイルに避けている
このフォークでは辞書ファイルをngramサイズで分けることで辞書のコスト設定の前準備をしている　

### 関連するissue

https://github.com/VOICEVOX/voicevox_engine/issues/1486#issuecomment-2910970470 より  



pyopenjtalk-plusの機能の
複数の読み方をする漢字の読みに対し SudachiPy で形態素解析を行い、得られた結果を使い OpenJTalk から返された list[NJDFeature] 内の値を補正する実装
ですが

背景として  
openjtalkの辞書はunidic-csj(たぶん2.12.2)とnaist-jdicをマージして使われており、形態素解析、短単位解析能力が機能しなくなっています。

（ここからopenjatlkのmecabの話です）特にマージ後に品詞情報と単語コストをちゃんと調整されていないので、mecab本来の機能の文脈によって違う品詞で判定する（文脈によって違う読みを与える）が壊れていて、どの文脈でも同じ読みが大体出ます。

pyopenjtalk-plusでは、それを解決するために入力文をsudachiとsudachi辞書を使ってもう一度形態素解析し、読みが違ったら上書きする処理があります。
（個人的にはsudachiではなく短単位辞書のunidicで同じような処理をした方がいいとは思っていますが、sudachiでも機能はします）